{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset is:(740, 22)\n",
      "Type of features is:\n",
      "ID                                   int64\n",
      "Reason for absence                   int64\n",
      "Month of absence                     int64\n",
      "Day of the week                      int64\n",
      "Seasons                              int64\n",
      "Transportation expense               int64\n",
      "Distance from Residence to Work      int64\n",
      "Service time                         int64\n",
      "Age                                  int64\n",
      "Work load Average/day              float64\n",
      "Hit target                           int64\n",
      "Disciplinary failure                 int64\n",
      "Education                            int64\n",
      "Son                                  int64\n",
      "Social drinker                       int64\n",
      "Social smoker                        int64\n",
      "Pet                                  int64\n",
      "Weight                               int64\n",
      "Height                               int64\n",
      "Body mass index                      int64\n",
      "Absenteeism time in hours            int64\n",
      "Absenteeism category                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "file = 'dataset/Absenteeism_at_work.csv'\n",
    "\n",
    "# Load the dataset as DataFrame in Pandas and then convert to matrix\n",
    "data = load_dataset(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent_train 60\n",
      "percent_test 20\n",
      "percent_validation 20\n",
      "Number of training examples: m_train = 444\n",
      "Number of testing examples: m_test = 148\n",
      "Number of validation examples: m_validation = 148\n",
      "Number of features: n_features = 21\n",
      "training_x shape (21, 444)\n",
      "training_y shape (1, 444)\n",
      "testing_x shape (21, 148)\n",
      "testing_y shape (1, 148)\n",
      "validation_x shape (21, 148)\n",
      "validation_y shape (1, 148)\n",
      "Original Data:\n",
      "[[ 5. 23. 10. ... 38.  2.  0.]\n",
      " [33. 23.  3. ... 32.  2.  0.]\n",
      " [28. 14. 11. ... 24.  3.  0.]\n",
      " ...\n",
      " [29. 22.  5. ... 28.  8.  1.]\n",
      " [36. 23. 12. ... 31.  2.  0.]\n",
      " [22. 23.  8. ... 19.  1.  0.]]\n",
      "____________________________________________________________\n",
      "X:\n",
      "Training:\n",
      "\n",
      "[[0.00061554 0.00406254 0.003447   ... 0.00418565 0.00160039 0.00221593]\n",
      " [0.00273386 0.00273386 0.00166409 ... 0.00332818 0.00273386 0.00023773]\n",
      " [0.00359842 0.00107953 0.00395826 ... 0.00395826 0.00359842 0.00395826]\n",
      " ...\n",
      " [0.00218298 0.00215683 0.00220912 ... 0.00224834 0.00220912 0.00237905]\n",
      " [0.00319408 0.00268975 0.00201732 ... 0.00235353 0.00210137 0.00210137]\n",
      " [0.00066556 0.00066556 0.00099834 ... 0.00099834 0.00099834 0.00798669]]\n",
      "Testing:\n",
      "\n",
      "[[0.00550531 0.01337004 0.00668502 ... 0.00275265 0.0086512  0.00117971]\n",
      " [0.00453594 0.00662945 0.00872296 ... 0.         0.00453594 0.0094208 ]\n",
      " [0.00300601 0.01202405 0.00801603 ... 0.01102204 0.00300601 0.00200401]\n",
      " ...\n",
      " [0.00772444 0.00677859 0.00669977 ... 0.00662095 0.00673918 0.00669977]\n",
      " [0.00639386 0.00716113 0.0056266  ... 0.00613811 0.00485934 0.00792839]\n",
      " [0.02362205 0.05511811 0.00098425 ... 0.         0.00787402 0.00295276]]\n",
      "Validation:\n",
      "\n",
      "[[0.01349831 0.0056243  0.01049869 ... 0.01087364 0.01349831 0.00824897]\n",
      " [0.00476029 0.00238014 0.00782047 ... 0.00748045 0.00782047 0.00782047]\n",
      " [0.00442968 0.00996678 0.01328904 ... 0.0055371  0.01328904 0.00885936]\n",
      " ...\n",
      " [0.00698313 0.00670851 0.00663005 ... 0.00714005 0.00698313 0.00670851]\n",
      " [0.00788002 0.00635486 0.00610066 ... 0.00711744 0.00788002 0.00482969]\n",
      " [0.00181324 0.03626473 0.00271985 ... 0.00725295 0.00181324 0.00090662]]\n",
      "__________________________________\n",
      "Y\n",
      "Training:\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      "  1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      "  1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0.\n",
      "  0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      "  0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1.\n",
      "  0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1.\n",
      "  0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      "  1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.\n",
      "  1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      "  0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.]]\n",
      "Testing:\n",
      "\n",
      "[[1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      "  1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      "  0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      "  0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      "  0. 0. 1. 0.]]\n",
      "Validation:\n",
      "\n",
      "[[0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      "  1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1.\n",
      "  1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      "  0. 1. 0. 0.]]\n",
      "__________________________________\n"
     ]
    }
   ],
   "source": [
    "# Randomly distribute data into training, testing and validation classes. We use 60-20-20 distribution\n",
    "un_training_x, training_y, un_testing_x, testing_y, un_validation_x, validation_y = split_random(data, percent_train=60, percent_test=20)\n",
    "\n",
    "# Lets normalize our X data\n",
    "training_x, testing_x, validation_x = normalize_data(un_training_x, un_testing_x, un_validation_x)\n",
    "\n",
    "# We can print the X data, to be sure that we have the normalized data in the range of -1 to 1\n",
    "print(\"X:\")\n",
    "print_normalized_data(training_x, testing_x, validation_x)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "\n",
    "# Lets print the Y class, to be sure that we have a mix of positive and negative class\n",
    "print(\"Y\")\n",
    "print_normalized_data(training_y, testing_y, validation_y)\n",
    "print(\"__________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_accuracy(predY, Y):\n",
    "    return (100 - np.mean(np.abs(predY - Y)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
       "      fit_intercept=True, max_iter=10, n_iter=None, n_iter_no_change=5,\n",
       "      n_jobs=None, penalty=None, random_state=42, shuffle=True, tol=None,\n",
       "      validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  65.09009009009009\n",
      "Test accuracy:  58.78378378378378\n",
      "Validation accuracy:  62.16216216216216\n"
     ]
    }
   ],
   "source": [
    "# Predict test/train/dev set examples \n",
    "Y_prediction_train = clf.predict(training_x.T)\n",
    "Y_prediction_test = clf.predict(testing_x.T)\n",
    "Y_prediction_dev = clf.predict(validation_x.T)\n",
    "\n",
    "# Lets perform prediction on train, test and dev sets\n",
    "\n",
    "acc_train = compute_accuracy(Y_prediction_train, training_y)\n",
    "acc_test = compute_accuracy(Y_prediction_test, testing_y)\n",
    "acc_dev = compute_accuracy(Y_prediction_dev, validation_y)\n",
    "\n",
    "#acc_data.append([\"Logistic Regression (sklearn)\", acc_train, acc_test, acc_dev])\n",
    "\n",
    "# Print train/test/dev Errors\n",
    "print(\"Train accuracy: \", compute_accuracy(Y_prediction_train, training_y))\n",
    "print(\"Test accuracy: \", compute_accuracy(Y_prediction_test, testing_y))\n",
    "print(\"Validation accuracy: \", compute_accuracy(Y_prediction_dev, validation_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights between input and first hidden layer:\n",
      "[[-0.07972265  0.21168084 -0.48027439 -0.18991265 -0.33938583]\n",
      " [-0.39166826 -0.30143132 -0.1483804  -0.09918256  0.0372939 ]\n",
      " [-0.07763537  0.17795307 -0.28395298  0.36328334 -0.45407114]\n",
      " [ 0.16377982 -0.07945095  0.05638734 -0.34550492 -0.29005459]\n",
      " [ 0.28894592  0.449891   -0.17925618  0.18477752  0.36162286]\n",
      " [ 0.37912567 -0.39867647 -0.44286167 -0.31721655  0.36330742]\n",
      " [-0.38589573 -0.07579731  0.43992586  0.03186416  0.1843495 ]\n",
      " [-0.17724678  0.17918423  0.32149782 -0.46281347  0.24033079]\n",
      " [ 0.46968236  0.23842976 -0.21094251  0.27793048 -0.38120797]\n",
      " [-0.05006226  0.39256571 -0.19828904 -0.20389878 -0.35545691]\n",
      " [-0.46177711  0.17181956 -0.27705864 -0.2252554  -0.00809624]\n",
      " [-0.42911522  0.07120986 -0.33941207  0.08580195  0.19192155]\n",
      " [-0.38206457 -0.0825723   0.18677356 -0.08245386 -0.43239056]\n",
      " [ 0.03448814  0.15736874  0.01430499  0.42715266  0.08315936]\n",
      " [ 0.38757587 -0.3483029  -0.34657194  0.29533188 -0.09830888]\n",
      " [-0.32151716  0.4107368  -0.14626177  0.24097238  0.21713176]\n",
      " [ 0.36826844  0.11882037  0.2410976  -0.14517372 -0.22104605]\n",
      " [ 0.38035503 -0.06908772  0.4466037   0.15702945  0.11692142]\n",
      " [-0.37013995  0.43185514 -0.04812285  0.07531428 -0.08825927]\n",
      " [-0.25265621  0.38755436  0.07078893 -0.47762656  0.11254915]\n",
      " [-0.16655413  0.02599657  0.37080103 -0.13713073  0.39250772]]\n",
      "\n",
      "weights between first hidden and second hidden layer:\n",
      "[[-0.60670725 -0.67189377]\n",
      " [ 0.80101084  0.36443628]\n",
      " [-0.80361121  0.47302547]\n",
      " [ 0.47008717  0.78328893]\n",
      " [ 0.39166759 -0.69571472]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  65.09009009009009\n",
      "Test accuracy:  58.78378378378378\n",
      "Validation accuracy:  62.16216216216216\n"
     ]
    }
   ],
   "source": [
    "# Predict test/train/dev set examples \n",
    "Y_prediction_train = clf.predict(training_x.T)\n",
    "Y_prediction_test = clf.predict(testing_x.T)\n",
    "Y_prediction_dev = clf.predict(validation_x.T)\n",
    "\n",
    "# Lets perform prediction on train, test and dev sets\n",
    "\n",
    "acc_train = compute_accuracy(Y_prediction_train, training_y)\n",
    "acc_test = compute_accuracy(Y_prediction_test, testing_y)\n",
    "acc_dev = compute_accuracy(Y_prediction_dev, validation_y)\n",
    "\n",
    "#acc_data.append([\"Logistic Regression (sklearn)\", acc_train, acc_test, acc_dev])\n",
    "\n",
    "# Print train/test/dev Errors\n",
    "print(\"Train accuracy: \", compute_accuracy(Y_prediction_train, training_y))\n",
    "print(\"Test accuracy: \", compute_accuracy(Y_prediction_test, testing_y))\n",
    "print(\"Validation accuracy: \", compute_accuracy(Y_prediction_dev, validation_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
